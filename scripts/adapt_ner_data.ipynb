{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data prepping\n",
    "\n",
    "This notebook aims to transform our NER data to the appropriate format for K-Adapter, in particular the factual adapter. The linguistic adapter makes much more sense in the paper because they are working with general knowledge. \n",
    "\n",
    "In the paper they used Wikidata for the factual adapter, and BookCorpus for the linguistic adapter. The wikidata they use contains 2 (related) entities per input sentence. Unfortunately, we probably will not have such data, but we could however force inject related entities together in a sentence. Or at least try!\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "\n",
    "name = \"dev\"\n",
    "ner_data = pd.read_csv(f\"data/ner_data/ser_bus_spc/en/{name}/data.csv\", index_col=0)\n",
    "ner_data[\"text\"] = ner_data[\"text\"].apply(lambda row: literal_eval(row))\n",
    "ner_data[\"tag\"] = ner_data[\"tag\"].apply(lambda row: literal_eval(row))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "ner_data.iloc[1][\"text\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['in',\n",
       " 'humans',\n",
       " 'methemoglobin',\n",
       " 'formation',\n",
       " 'is',\n",
       " 'very',\n",
       " 'rare',\n",
       " 'at',\n",
       " 'therapeutic',\n",
       " 'doses',\n",
       " 'and',\n",
       " 'overdoses',\n",
       " 'of',\n",
       " 'acetaminophen']"
      ]
     },
     "metadata": {},
     "execution_count": 279
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "source": [
    "def create_lookup_table(spo_path:str, predicate:bool = False):\n",
    "    lookup_table = {}\n",
    "    print(\"[KnowledgeGraph] Loading spo from {}\".format(spo_path))\n",
    "    with open(spo_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                subj, pred, obje = line.strip().split(\"\\t\")    \n",
    "            except:\n",
    "                print(\"[KnowledgeGraph] Bad spo:\", line)\n",
    "            if predicate:\n",
    "                value = pred + \" \" + obje\n",
    "            else:\n",
    "                value = obje\n",
    "            if subj in lookup_table.keys():\n",
    "                lookup_table[subj].append(value)\n",
    "            else:\n",
    "                lookup_table[subj] = list([value])\n",
    "    return lookup_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "source": [
    "clinia_kg = create_lookup_table(spo_path=\"data/custom_taxo/clinia_kg.spo\", predicate = False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[KnowledgeGraph] Loading spo from data/custom_taxo/clinia_kg.spo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "source": [
    "import numpy as np\n",
    "def get_entity_pos(tags):\n",
    "    entities_pos = []\n",
    "    cont = 0\n",
    "    i_max = len(tags)\n",
    "    for i,tag in enumerate(tags):\n",
    "        if tag !=\"O\" and cont == 0:\n",
    "            index = i # remember the\n",
    "            if i == (i_max -1):\n",
    "                # case where entity is one word long and at the end\n",
    "                entities_pos.append((index, index + cont))\n",
    "            cont+=1\n",
    "        elif cont !=0 and tag !=\"O\" and i != (i_max-1):\n",
    "            # continuing an entity case\n",
    "            cont+=1\n",
    "        elif cont !=0 and tag == \"O\":\n",
    "            # standard case\n",
    "            entities_pos.append((index, index + cont -1))\n",
    "            cont = 0\n",
    "        elif cont!=0 and i == (i_max-1):\n",
    "            # case where entity is more than one word long and at the end\n",
    "            entities_pos.append((index, index + cont))\n",
    "    return entities_pos\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "source": [
    "import re\n",
    "with open(\"data/custom_taxo/graph_triples.nt\", 'r', encoding=\"utf-8\") as g:\n",
    "    graphs = {}\n",
    "    for line in g:\n",
    "        try:\n",
    "            subj, relation, parent = re.findall(r'\"(.*?)\"', line)\n",
    "        except:\n",
    "            print(\"Bad formatting, skipping.\")\n",
    "\n",
    "        if subj not in graphs.keys():\n",
    "            graphs[subj] = {}\n",
    "            graphs[subj][\"relations\"] = [(relation, parent)]\n",
    "        else:\n",
    "            graphs[subj][\"relations\"].append((relation, parent))\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "ner_data[\"entity_pos\"] = ner_data[\"tag\"].apply(lambda row: get_entity_pos(row))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "import random\n",
    "def process_data(df:pd.DataFrame, taxo:dict):\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        exs = []\n",
    "        for ent_pos in row[\"entity_pos\"]:\n",
    "            ex = {}\n",
    "            ex[\"token\"] = row[\"text\"]\n",
    "            ex[\"subj_start\"] = ent_pos[0] # we are not guarenteed to have a subject object relationship in a single sentence\n",
    "            ex[\"subj_end\"] = ent_pos[1]\n",
    "\n",
    "            entity = \" \".join(ex[\"token\"][ent_pos[0]: ent_pos[1]+1])\n",
    "            #relation = taxo[entity][\"relation\"] # need relation label of course\n",
    "            try:\n",
    "                relation, parent = random.choice(taxo[entity][\"relations\"]) # \n",
    "            except KeyError:\n",
    "                relation = \"no_relation\"\n",
    "                parent = \"no_parent\"\n",
    "            ex[\"relation\"] = relation\n",
    "\n",
    "\n",
    "            #related_entity = taxo[entity][\"related_entity\"] # \n",
    "            #ex[\"token\"].insert(ent_pos[0]+1, parent) # TODO: what if the parent has multiple words??\n",
    "            ex[\"token\"][ent_pos[1]+1:ent_pos[1]+1] = parent.split()\n",
    "            ex[\"obj_start\"] = ent_pos[1] + 1\n",
    "            ex[\"obj_end\"] = ent_pos[1] + len(parent.split())\n",
    "            #ex[\"obj_start\"] = 0\n",
    "            #ex[\"obj_end\"] = 0\n",
    "            #coudld have labels for subj and obj\n",
    "            # we probably need to add a mask / visibility matrix for this last part of input we added, like in KBERT,\n",
    "            # because we do not want to corrupt the sentence contextual representation with it. It probably will not make much sense. \n",
    "            exs.append(ex)\n",
    "        examples.extend(exs)\n",
    "    return examples\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "import json\n",
    "ner_data_copy = ner_data.copy(deep = True)\n",
    "exs = process_data(ner_data_copy, graphs)\n",
    "with open(f\"data/ner_data/custom_data/{name}.json\", \"w\") as f:\n",
    "    json.dump(exs, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "source": [
    "def get_labels(examples):\n",
    "    labels2id = {}\n",
    "    i=0\n",
    "    for entry in examples:\n",
    "        relation = entry[\"relation\"]\n",
    "        if relation not in labels2id.keys():\n",
    "            labels2id[relation] = i\n",
    "            i+=1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    id2labels = {v:k for k,v in labels2id.items()}\n",
    "    return labels2id, id2labels\n",
    "labels2id, id2labels = get_labels(exs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "list(labels2id.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['no_relation',\n",
       " 'facet',\n",
       " 'counseling',\n",
       " 'anesthesia',\n",
       " 'treatment',\n",
       " 'diagnostics',\n",
       " 'profession',\n",
       " 'consultation',\n",
       " 'surgical procedures',\n",
       " 'childbirth',\n",
       " 'education',\n",
       " 'technical act',\n",
       " 'medical documents',\n",
       " 'prescription']"
      ]
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "with open(\"data/ner_data/custom_data/relations.json\", \"w\") as f:\n",
    "    json.dump(labels2id, f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('k-adapter-yjoD7LG2-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "ac13078e819976022ed1dbb11dbcf70da4eb488025c72dccf5a05e9cff8c985d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}