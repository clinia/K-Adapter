{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data prepping for Relation classification and Entity Typing\n",
    "\n",
    "This notebook aims to transform our NER data to the appropriate format for K-Adapter, in particular the factual adapter. The linguistic adapter makes much more sense in the paper because they are working with general knowledge. \n",
    "\n",
    "In the paper they used Wikidata for the factual adapter, and BookCorpus for the linguistic adapter. The wikidata they use contains 2 (related) entities per input sentence. Unfortunately, we probably will not have such data, but we could however force inject related entities together in a sentence. Or at least try!\n",
    "\n",
    "The data for entity typing is relatively similar, but we simply add a label for each entity. Since we will be using our NER data, the labels will be BUS, SER, SPC.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "\n",
    "name = \"dev\"\n",
    "ner_data = pd.read_csv(f\"data/ner_data/ser_bus/en/{name}/data.csv\", index_col=0)\n",
    "ner_data[\"text\"] = ner_data[\"text\"].apply(lambda row: literal_eval(row))\n",
    "ner_data[\"tag\"] = ner_data[\"tag\"].apply(lambda row: literal_eval(row))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def create_lookup_table(spo_path:str, predicate:bool = False):\n",
    "    lookup_table = {}\n",
    "    print(\"[KnowledgeGraph] Loading spo from {}\".format(spo_path))\n",
    "    with open(spo_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                subj, pred, obje = line.strip().split(\"\\t\")    \n",
    "            except:\n",
    "                print(\"[KnowledgeGraph] Bad spo:\", line)\n",
    "            if predicate:\n",
    "                value = pred + \" \" + obje\n",
    "            else:\n",
    "                value = obje\n",
    "            if subj in lookup_table.keys():\n",
    "                lookup_table[subj].append(value)\n",
    "            else:\n",
    "                lookup_table[subj] = list([value])\n",
    "    return lookup_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "clinia_kg = create_lookup_table(spo_path=\"data/custom_taxo/clinia_kg.spo\", predicate = False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[KnowledgeGraph] Loading spo from data/custom_taxo/clinia_kg.spo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "def get_entity_pos(tags):\n",
    "    entities_pos = []\n",
    "    cont = 0\n",
    "    i_max = len(tags)\n",
    "    for i,tag in enumerate(tags):\n",
    "        if tag !=\"O\" and cont == 0:\n",
    "            index = i # remember the\n",
    "            if i == (i_max -1):\n",
    "                # case where entity is one word long and at the end\n",
    "                entities_pos.append((index, index + cont))\n",
    "            cont+=1\n",
    "        elif cont !=0 and tag !=\"O\" and i != (i_max-1):\n",
    "            # continuing an entity case\n",
    "            cont+=1\n",
    "        elif cont !=0 and tag == \"O\":\n",
    "            # standard case\n",
    "            entities_pos.append((index, index + cont -1))\n",
    "            cont = 0\n",
    "        elif cont!=0 and i == (i_max-1):\n",
    "            # case where entity is more than one word long and at the end\n",
    "            entities_pos.append((index, index + cont))\n",
    "    return entities_pos\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import re\n",
    "with open(\"data/custom_taxo/graph_triples.nt\", 'r', encoding=\"utf-8\") as g:\n",
    "    graphs = {}\n",
    "    for line in g:\n",
    "        try:\n",
    "            subj, relation, parent = re.findall(r'\"(.*?)\"', line)\n",
    "        except:\n",
    "            print(\"Bad formatting, skipping.\")\n",
    "            \n",
    "        if relation == \"product\":\n",
    "            print(subj, relation, parent)\n",
    "\n",
    "        if subj not in graphs.keys():\n",
    "            graphs[subj] = {}\n",
    "            graphs[subj][\"relations\"] = [(relation, parent)]\n",
    "        else:\n",
    "            graphs[subj][\"relations\"].append((relation, parent))\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "portable oxygen cylinder exchange product sleep medicine\n",
      "neurolens product optometry\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(graphs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ner_data[\"entity_pos\"] = ner_data[\"tag\"].apply(lambda row: get_entity_pos(row))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import random\n",
    "def process_data(df:pd.DataFrame, taxo:dict):\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        exs = []\n",
    "        for ent_pos in row[\"entity_pos\"]:\n",
    "            ex = {}\n",
    "            ex[\"sent\"] = \" \".join(row[\"text\"])\n",
    "            ex[\"subj_start\"] = ent_pos[0] # we are not guarenteed to have a subject object relationship in a single sentence\n",
    "            ex[\"subj_end\"] = ent_pos[1]\n",
    "\n",
    "            #entity = \" \".join(ex[\"token\"][ent_pos[0]: ent_pos[1]+1])\n",
    "            # #relation = taxo[entity][\"relation\"] # need relation label of course\n",
    "            # try:\n",
    "            #     relation, parent = random.choice(taxo[entity][\"relations\"]) # \n",
    "            # except KeyError:\n",
    "            #     relation = \"no_relation\"\n",
    "            #     parent = \"no_parent\"\n",
    "            # ex[\"relation\"] = relation\n",
    "\n",
    "            #ex[\"token\"][ent_pos[1]+1:ent_pos[1]+1] = parent.split() # uncomment for RC\n",
    "            #ex[\"obj_start\"] = ent_pos[1] + 1\n",
    "            #ex[\"obj_end\"] = ent_pos[1] + len(parent.split())\n",
    "\n",
    "            ex[\"subj_label\"] = re.sub(r\"\\w\\-\", \"\", row[\"tag\"][ent_pos[0]])  # add labrls for ET\n",
    "\n",
    "            # we probably need to add a mask / visibility matrix for this last part of input we added, like in KBERT,\n",
    "            # because we do not want to corrupt the sentence contextual representation with it. It probably will not make much sense. \n",
    "            exs.append(ex)\n",
    "        examples.extend(exs)\n",
    "    return examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import random\n",
    "def process_data(df:pd.DataFrame, taxo:dict):\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        exs = []\n",
    "        for ent_pos in row[\"entity_pos\"]:\n",
    "            ex = {}\n",
    "            ex[\"token\"] = row[\"text\"]\n",
    "            ex[\"subj_start\"] = ent_pos[0] # we are not guarenteed to have a subject object relationship in a single sentence\n",
    "            ex[\"subj_end\"] = ent_pos[1]\n",
    "\n",
    "            entity = \" \".join(ex[\"token\"][ent_pos[0]: ent_pos[1]+1])\n",
    "            #relation = taxo[entity][\"relation\"] # need relation label of course\n",
    "            try:\n",
    "                relation, parent = random.choice(taxo[entity][\"relations\"]) # \n",
    "            except KeyError:\n",
    "                relation = \"no_relation\"\n",
    "                parent = \"no_parent\"\n",
    "            ex[\"relation\"] = relation\n",
    "\n",
    "            ex[\"token\"][ent_pos[1]+1:ent_pos[1]+1] = parent.split() # uncomment for RC\n",
    "            ex[\"obj_start\"] = ent_pos[1] + 1\n",
    "            ex[\"obj_end\"] = ent_pos[1] + len(parent.split())\n",
    "\n",
    "            ex[\"subj_label\"] = re.sub(r\"\\w\\-\", \"\", row[\"tag\"][ent_pos[0]])  # add labrls for ET\n",
    "\n",
    "            # we probably need to add a mask / visibility matrix for this last part of input we added, like in KBERT,\n",
    "            # because we do not want to corrupt the sentence contextual representation with it. It probably will not make much sense. \n",
    "            exs.append(ex)\n",
    "        examples.extend(exs)\n",
    "    return examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import json\n",
    "ner_data_copy = ner_data.copy(deep = True)\n",
    "exs = process_data(ner_data_copy, graphs)\n",
    "with open(f\"data/ner_data/rc_data/{name}.json\", \"w\") as f:\n",
    "    json.dump(exs, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_labels(examples):\n",
    "    labels2id = {}\n",
    "    i=0\n",
    "    for entry in examples:\n",
    "        relation = entry[\"relation\"]\n",
    "        if relation not in labels2id.keys():\n",
    "            labels2id[relation] = i\n",
    "            i+=1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    id2labels = {v:k for k,v in labels2id.items()}\n",
    "    return labels2id, id2labels\n",
    "labels2id, id2labels = get_labels(exs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "with open(\"data/ner_data/custom_data/relations.json\", \"w\") as f:\n",
    "    json.dump(labels2id, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entity MLM\n",
    "\n",
    "Another pretraining task that could be interesting would be a form of masked language model that would mask entire entities instead of single tokens. This could be seen as a form contextual pretraining specialized over our data.\n",
    "\n",
    "Let's first explore the RoBERTa tokenizer and how we could efficiently mask entities(recall that they vary in # of words).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from pytorch_transformers import RobertaTokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "import torch\n",
    "import pandas as pd, re\n",
    "from ast import literal_eval\n",
    "\n",
    "name = \"dev\"\n",
    "ner_data = pd.read_csv(f\"data/ner_data/ser_bus_spc/en/{name}/data.csv\", index_col=0)\n",
    "ner_data[\"text\"] = ner_data[\"text\"].apply(lambda row: literal_eval(row))\n",
    "ner_data[\"tag\"] = ner_data[\"tag\"].apply(lambda row: literal_eval(row))\n",
    "\n",
    "with open(\"data/custom_taxo/graph_triples.nt\", 'r', encoding=\"utf-8\") as g:\n",
    "    graphs = {}\n",
    "    for line in g:\n",
    "        try:\n",
    "            subj, relation, parent = re.findall(r'\"(.*?)\"', line)\n",
    "        except:\n",
    "            print(\"Bad formatting, skipping.\")\n",
    "\n",
    "        if subj not in graphs.keys():\n",
    "            graphs[subj] = {}\n",
    "            graphs[subj][\"relations\"] = [(relation, parent)]\n",
    "        else:\n",
    "            graphs[subj][\"relations\"].append((relation, parent))\n",
    "\n",
    "ner_data[\"entity_pos\"] = ner_data[\"tag\"].apply(lambda row: get_entity_pos(row))\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "source": [
    "def process_data(df:pd.DataFrame):\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        exs = []\n",
    "        for ent_pos in row[\"entity_pos\"]:\n",
    "            ex = {}\n",
    "            ex[\"token\"] = row[\"text\"]\n",
    "            ex[\"label\"] = row[\"tag\"]\n",
    "            exs.append(ex)\n",
    "        examples.extend(exs)\n",
    "    return examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "source": [
    "import json\n",
    "exs = process_data(ner_data_copy)\n",
    "with open(f\"data/ner_data/mlm_data/{name}.json\", \"w\") as f:\n",
    "    json.dump(exs, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\",add_prefix_space=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "text = ner_data.iloc[3][\"text\"]\n",
    "tags = ner_data.iloc[3][\"tag\"]\n",
    "inputs = tokenizer(text,return_tensors=\"pt\", is_split_into_words=True, padding=\"max_length\", max_length=32, add_special_tokens=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "source": [
    "word_ids = inputs.word_ids()\n",
    "word_ids = torch.tensor([word_id if word_id is not None else -1 for word_id in word_ids])\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "ent_pos = ner_data.iloc[3][\"entity_pos\"] # coin flip here, with a bonus if there is only one entity.\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "source": [
    "ent_pos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(9, 10), (14, 14)]"
      ]
     },
     "metadata": {},
     "execution_count": 568
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "source": [
    "import torch\n",
    "mask = torch.rand(input_ids.size()) < 0.15\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "source": [
    "entity_masks = []\n",
    "for ent in ent_pos:\n",
    "    tmp1 = word_ids >= ent[0]\n",
    "    #print(tmp1)\n",
    "    tmp2 = word_ids <= ent[1]\n",
    "    #print(tmp2)\n",
    "    tmp = torch.logical_and(tmp1, tmp2)\n",
    "    entity_masks.append(tmp.int())\n",
    "\n",
    "entity_mask = sum(entity_masks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "source": [
    "entity_masks"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)]"
      ]
     },
     "metadata": {},
     "execution_count": 646
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "source": [
    "new = entity_mask * input_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "source": [
    "tokenizer.convert_ids_to_tokens(new[new!=0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Ġbreastfeeding', 'Ġsupport', 'Ġcounselling']"
      ]
     },
     "metadata": {},
     "execution_count": 608
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "source": [
    "(input_ids * entity_mask)[:,11:13]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[24161,   323]])"
      ]
     },
     "metadata": {},
     "execution_count": 644
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entity_index = torch.nonzero(entity_mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "source": [
    "chosen_tensor = (torch.rand(input_ids.size()) < 0.5).squeeze()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "source": [
    "torch.logical_and(chosen_tensor, entity_masks[0]).int()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "execution_count": 679
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "source": [
    "chosen_tensor"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ True, False, False,  True,  True, False,  True, False, False,  True,\n",
       "        False, False,  True,  True, False,  True, False,  True,  True,  True,\n",
       "         True, False, False,  True,  True,  True, False,  True, False, False,\n",
       "         True, False])"
      ]
     },
     "metadata": {},
     "execution_count": 680
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "source": [
    "if torch.logical_and(chosen_tensor, entity_masks[0]).int().any():\n",
    "    chosen_tensor[entity_masks[0].bool()] = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "source": [
    "chosen_tensor[entity_masks[0].bool()]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "metadata": {},
     "execution_count": 685
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Named Entity Recognition using their data pipeline\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "import random\n",
    "\n",
    "def process_data(df:pd.DataFrame):\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        ex = {}\n",
    "        ex[\"token\"] = row[\"text\"]\n",
    "\n",
    "        #ex[\"token\"][ent_pos[1]+1:ent_pos[1]+1] = parent.split() # uncomment for RC\n",
    "        #ex[\"obj_start\"] = ent_pos[1] + 1\n",
    "        #ex[\"obj_end\"] = ent_pos[1] + len(parent.split())\n",
    "\n",
    "        ex[\"labels\"] = row[\"tag\"]\n",
    "\n",
    "        # we probably need to add a mask / visibility matrix for this last part of input we added, like in KBERT,\n",
    "        # because we do not want to corrupt the sentence contextual representation with it. It probably will not make much sense. \n",
    "        examples.append(ex)\n",
    "    return examples\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"data/ner_data/finetuning/\"\n",
    "if not os.path.exists(root_path):\n",
    "    os.makedirs(root_path)\n",
    "    \n",
    "for name in [\"dev\", \"test\", \"train\"]:\n",
    "    ner_data = pd.read_csv(f\"data/ner_data/ser_bus/en/{name}/data.csv\", index_col=0)\n",
    "    ner_data[\"text\"] = ner_data[\"text\"].apply(lambda row: literal_eval(row))\n",
    "    ner_data[\"tag\"] = ner_data[\"tag\"].apply(lambda row: literal_eval(row))\n",
    "    exs = process_data(ner_data)\n",
    "    with open(\"{}/{}.json\".format(root_path, name), \"w\") as f:\n",
    "        json.dump(exs, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"data/ner_data/ser_bus_spc/en/data.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from yaml_preprocessing import adapt_input_data\n",
    "\n",
    "df = adapt_input_data(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['otorhinolaryngology', 'consultation']</td>\n",
       "      <td>['B-SER', 'I-SER']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['green', 'bay', 'medical', 'center']</td>\n",
       "      <td>['O', 'O', 'B-BUS', 'I-BUS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['child', 'mental', 'health', 'counseling']</td>\n",
       "      <td>['O', 'B-SER', 'I-SER', 'I-SER']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"children's\", 'mental', 'health', 'counseling']</td>\n",
       "      <td>['O', 'B-SER', 'I-SER', 'I-SER']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['clsc', 'ahuntsic']</td>\n",
       "      <td>['B-BUS', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>['jean', 'coutu', '-', 'jean-philippe', 'roy',...</td>\n",
       "      <td>['B-BUS', 'I-BUS', 'O', 'B-BUS', 'I-BUS', 'B-S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>['treatment', 'for', 'substance', 'abuse', 'di...</td>\n",
       "      <td>['B-SER', 'O', 'B-SER', 'I-SER', 'I-SER']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>['community', 'care', 'walk-in', 'clinic', 'in...</td>\n",
       "      <td>['B-BUS', 'I-BUS', 'B-BUS', 'I-BUS', 'O', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>['specialized', 'accompaniment', 'for', 'adhd'...</td>\n",
       "      <td>['B-SER', 'I-SER', 'I-SER', 'I-SER', 'B-BUS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>['perineal', 'and', 'pelvic', 'rehabilitation'...</td>\n",
       "      <td>['B-SER', 'I-SER', 'I-SER', 'I-SER', 'B-BUS', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0              ['otorhinolaryngology', 'consultation']   \n",
       "1                ['green', 'bay', 'medical', 'center']   \n",
       "2          ['child', 'mental', 'health', 'counseling']   \n",
       "3     [\"children's\", 'mental', 'health', 'counseling']   \n",
       "4                                 ['clsc', 'ahuntsic']   \n",
       "..                                                 ...   \n",
       "126  ['jean', 'coutu', '-', 'jean-philippe', 'roy',...   \n",
       "127  ['treatment', 'for', 'substance', 'abuse', 'di...   \n",
       "128  ['community', 'care', 'walk-in', 'clinic', 'in...   \n",
       "129  ['specialized', 'accompaniment', 'for', 'adhd'...   \n",
       "130  ['perineal', 'and', 'pelvic', 'rehabilitation'...   \n",
       "\n",
       "                                                   tag  \n",
       "0                                   ['B-SER', 'I-SER']  \n",
       "1                         ['O', 'O', 'B-BUS', 'I-BUS']  \n",
       "2                     ['O', 'B-SER', 'I-SER', 'I-SER']  \n",
       "3                     ['O', 'B-SER', 'I-SER', 'I-SER']  \n",
       "4                                       ['B-BUS', 'O']  \n",
       "..                                                 ...  \n",
       "126  ['B-BUS', 'I-BUS', 'O', 'B-BUS', 'I-BUS', 'B-S...  \n",
       "127          ['B-SER', 'O', 'B-SER', 'I-SER', 'I-SER']  \n",
       "128     ['B-BUS', 'I-BUS', 'B-BUS', 'I-BUS', 'O', 'O']  \n",
       "129      ['B-SER', 'I-SER', 'I-SER', 'I-SER', 'B-BUS']  \n",
       "130  ['B-SER', 'I-SER', 'I-SER', 'I-SER', 'B-BUS', ...  \n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('k-adapter-1d2mpP3z-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1172bf9f13a8094b792f4199a2964d700687e13b2160ddd0502a93c955444c42"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}