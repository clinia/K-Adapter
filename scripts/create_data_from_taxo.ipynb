{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation for relation classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This script aims to transform our graph triples into training files that can be ingested by the DataProcessors required for this repo. The files created are especially meant to be used for relation classification purposes. Because we don't want to use our NER training sentences, we are going to use the object and the subject of a triple in a next sentence prediction task (<s>object</s></s>subject</s>). The goal here will be to classify the relation between the two segments rather than telling if they are simply related. We hope it's going to help clustering realted triples together in the embedding space."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load graph triples\n",
    "with open(\"data/custom_taxo/clinia_triples.spo\", \"r\") as g:\n",
    "    graph = {}\n",
    "    for line in g:\n",
    "        try:\n",
    "            subj, relation, parent = line.strip().split(\"\\t\")\n",
    "        except:\n",
    "            print(\"Bad formatting, skipping.\")\n",
    "\n",
    "        if subj not in graph.keys():\n",
    "            graph[subj] = {}\n",
    "            graph[subj][\"relations\"] = [(relation, parent)]\n",
    "        else:\n",
    "            graph[subj][\"relations\"].append((relation, parent))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# List relations\n",
    "pred_set = set()\n",
    "for _, relations in graph.items():\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        pred_set.add(predicate)\n",
    "\n",
    "print(list(pred_set))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "# Split triples into train, dev and test sets\n",
    "graph_splits = dict()\n",
    "splits = [0.8, 0.1, 0.1]\n",
    "graph_splits[\"train\"] = dict(list(graph.items())[: int(len(graph) * splits[0])])\n",
    "graph_splits[\"dev\"] = dict(list(graph.items())[int(len(graph) * splits[0]) : int(len(graph) * (splits[0] + splits[1]))])\n",
    "graph_splits[\"test\"] = dict(list(graph.items())[int(len(graph) * (splits[0] + splits[1])) :])\n",
    "\n",
    "\n",
    "# Export files\n",
    "for name, graph in graph_splits.items():\n",
    "    print(len(graph))\n",
    "    with open(\"data/graph_data/rc_data/{}.json\".format(name), \"w\") as f:\n",
    "        json.dump(graph, f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('k-adapter-1d2mpP3z-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1172bf9f13a8094b792f4199a2964d700687e13b2160ddd0502a93c955444c42"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}