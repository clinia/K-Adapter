{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for relation classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script aims to transform our graph triples into training files that can be ingested by the DataProcessors required for this repo. The files created are especially meant to be used for relation classification purposes. Because we don't want to use our NER training sentences, we are going to use the object and the subject of a triple in a next sentence prediction task (<s>object</s></s>subject</s>). The goal here will be to classify the relation between the two segments rather than telling if they are simply related. We hope it's going to help clustering realted triples together in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph triples\n",
    "with open(\"data/custom_taxo/clinia_triples.spo\", \"r\") as g:\n",
    "    graph = {}\n",
    "    entities = set()\n",
    "    for line in g:\n",
    "        try:\n",
    "            subj, relation, obj = line.strip().split(\"\\t\")\n",
    "        except:\n",
    "            print(\"Bad formatting, skipping.\")\n",
    "        \n",
    "        entities.add(subj)\n",
    "        entities.add(obj)\n",
    "        if subj not in graph.keys():\n",
    "            graph[subj] = {}\n",
    "            graph[subj][\"relations\"] = [(relation, obj)]\n",
    "        else:\n",
    "            graph[subj][\"relations\"].append((relation, obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Add disjoint triples for negative sampling - FROM RANDOM WORDS\n",
    "\n",
    "with open(\"data/random_words/words.txt\", \"r\") as f:\n",
    "    words = set()\n",
    "    for line in f:\n",
    "        w = line.strip().lower()\n",
    "        if w not in entities:\n",
    "            words.add(w)\n",
    "\n",
    "words = list(words) # Sets are not subscriptable\n",
    "\n",
    "n_triples = 1000 # since we use 2 relations for each subj and we have about 5000 triples\n",
    "disjoint_triples = {}\n",
    "for _ in range(n_triples):\n",
    "    subj = random.choice(words)\n",
    "    obj1 = random.choice(words)\n",
    "    obj2 = random.choice(words)\n",
    "    \n",
    "    if subj != obj1 and subj != obj2 and obj1 != obj2:\n",
    "        disjoint_triples[subj] = {}\n",
    "        disjoint_triples[subj][\"relations\"] = [(\"disjoint with\", obj1), (\"disjoint with\", obj2)]\n",
    "\n",
    "    \n",
    "# Add disjoint triples to graph\n",
    "graph = {**graph, **disjoint_triples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Add disjoint triples for negative sampling - FROM TAXONOMY INSTANCES\n",
    "\n",
    "def verify_triple_existance(subj, obj, graph, reccursion_depth=0):\n",
    "    reccursion_depth += 1\n",
    "    if reccursion_depth == 3:\n",
    "        return False\n",
    "    elif subj not in graph.keys() and obj not in graph.keys():\n",
    "        # search the whole graph if obj and subj are related by a common subject\n",
    "        for s, rel in graph.items():\n",
    "            rel_obj = {item[1] for item in rel[\"relations\"] if item[0] != \"disjoint with\"}\n",
    "            if obj in rel_obj and subj in rel_obj:\n",
    "                return True\n",
    "        return False\n",
    "    elif subj not in graph.keys():\n",
    "            # Check if obj has subj as an object\n",
    "            return verify_triple_existance(obj, subj, graph, reccursion_depth)\n",
    "    elif subj == obj:\n",
    "        return True\n",
    "    else:\n",
    "        rel_obj = {item[1] for item in graph[subj][\"relations\"] if item[0] != \"disjoint with\"}\n",
    "        if obj not in rel_obj and verify_triple_existance(obj, subj, graph, reccursion_depth):\n",
    "            # Check wether obj is within objecs of that subject AND that the opposite triple either doesent exist OR doesnt contain the opposite triple\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "g = {\"test\": {\"relations\":[(\"rel\", \"bbb\"),(\"rel\", \"aaa\")]},\"ccc\": {\"relations\":[(\"rel\", \"ccc\"),(\"disjoint with\", \"ddd\")]}}\n",
    "\n",
    "print(verify_triple_existance(\"test\", \"ccc\", g, 0))\n",
    "\n",
    "words = list(entities) # Sets are not subscriptable\n",
    "\n",
    "n_triples = 2000 # since we use 2 relations for each subj and we have about 5000 triples\n",
    "disjoint_triples = {}\n",
    "for _ in range(n_triples):\n",
    "    subj, obj1, obj2 = random.sample(words, 3)\n",
    "    \n",
    "    # Verify that the triples dont exist\n",
    "    exist_1 = verify_triple_existance(subj, obj1, graph)\n",
    "    exist_2 = verify_triple_existance(subj, obj2, graph)\n",
    "    exist_3 = verify_triple_existance(obj1, obj2, graph)\n",
    "\n",
    "    if not exist_1 and not exist_2 and not exist_3:\n",
    "        if subj not in graph.keys():\n",
    "            graph[subj] = {}\n",
    "            graph[subj][\"relations\"] = [(\"disjoint with\", obj1), (\"disjoint with\", obj2)]\n",
    "        else:\n",
    "            graph[subj][\"relations\"].append((\"disjoint with\", obj1))\n",
    "            graph[subj][\"relations\"].append((\"disjoint with\", obj2))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is condition treated by institution organization', 'can be institution organization has intervention', 'is institution organization has intervention', 'is intervention of discipline', 'can be condition treated by institution organization', 'is discipline has intervention', 'is sign symptom of condition', 'pref label', 'alt label', 'can be discipline has intervention', 'is discipline treats sign symptom', 'scope', 'is target population of discipline', 'can be intervention of institution organization', 'is discipline has agent', 'feminin', 'is condition treated by intervention', 'is intervention of institution organization', 'hidden label', 'is condition has sign symptom', 'abbreviation', 'is drug treats condition', 'is subspecialty of discipline', 'is discipline of subspecialty', 'is sign symptom treated by discipline', 'is discipline concerned by concept', 'can be institution organization treating condition', 'is condition treated by discipline', 'is agent of discipline', 'is condition treated by drug', 'treats', 'is discipline treats condition', 'is institution organization treating condition', 'is concept concerns discipline', 'can be intervention of discipline', 'is discipline has target population', 'is target population of condition', 'is condition has target population']\n"
     ]
    }
   ],
   "source": [
    "# List relations\n",
    "pred_set = set()\n",
    "for _, relations in graph.items():\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        pred_set.add(predicate)\n",
    "\n",
    "print(list(pred_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21281\n",
      "{'is condition treated by discipline': 2610, 'is discipline treats condition': 2610, 'is condition treated by intervention': 2344, 'treats': 2344, 'is concept concerns discipline': 1997, 'is discipline concerned by concept': 1997, 'is condition treated by drug': 944, 'is drug treats condition': 944, 'can be intervention of discipline': 938, 'can be discipline has intervention': 938, 'alt label': 624, 'pref label': 374, 'hidden label': 272, 'is subspecialty of discipline': 224, 'is discipline of subspecialty': 224, 'is condition treated by institution organization': 168, 'is institution organization treating condition': 168, 'is discipline has agent': 150, 'is agent of discipline': 150, 'is discipline has intervention': 126, 'is intervention of discipline': 126, 'can be condition treated by institution organization': 122, 'can be institution organization treating condition': 122, 'is target population of discipline': 113, 'is discipline has target population': 113, 'is sign symptom of condition': 112, 'is condition has sign symptom': 112, 'abbreviation': 78, 'is target population of condition': 59, 'is condition has target population': 59, 'is institution organization has intervention': 22, 'is intervention of institution organization': 22, 'is discipline treats sign symptom': 21, 'is sign symptom treated by discipline': 21, 'can be institution organization has intervention': 12, 'can be intervention of institution organization': 12, 'scope': 7, 'feminin': 2}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# predicat distribution analysis\n",
    "pred_set = list()\n",
    "for _, relations in graph.items():\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        pred_set.append(predicate)\n",
    "        \n",
    "results = dict(Counter(pred_set))\n",
    "\n",
    "results = dict(sorted(results.items(), key=lambda item: item[1],reverse=True))\n",
    "total = sum([n for key, n in results.items()])\n",
    "\n",
    "print(total)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# predicat distribution analysis\n",
    "obj_set = list()\n",
    "for _, relations in graph.items():\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        obj_set.append(obj)\n",
    "        \n",
    "results = dict(Counter(obj_set))\n",
    "\n",
    "results = dict(sorted(results.items(), key=lambda item: item[1],reverse=True))\n",
    "total = sum([n for key, n in results.items()])\n",
    "\n",
    "print(total)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Instances set for visualization\n",
    "instance_set = set()\n",
    "for subj, relations in graph.items():\n",
    "    instance_set.add(subj)\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        instance_set.add(obj)\n",
    "        \n",
    "df = pd.DataFrame(instance_set)\n",
    "\n",
    "df.to_csv(\"tensorboard/data/facets/all/name.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n",
      "80\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Shuffle triples\n",
    "keys = list(graph.keys())\n",
    "random.shuffle(keys)\n",
    "\n",
    "shuffled_graph = {k:graph[k] for k in keys}\n",
    "\n",
    "# Split triples into train, dev and test sets\n",
    "graph_splits = dict()\n",
    "splits = [0.9, 0.1, 0.0]\n",
    "graph_splits[\"train\"] = dict(list(shuffled_graph.items())[: int(len(shuffled_graph) * splits[0])])\n",
    "graph_splits[\"dev\"] = dict(list(shuffled_graph.items())[int(len(shuffled_graph) * splits[0]) : int(len(shuffled_graph) * (splits[0] + splits[1]))])\n",
    "graph_splits[\"test\"] = dict(list(shuffled_graph.items())[int(len(shuffled_graph) * (splits[0] + splits[1])) :])\n",
    "\n",
    "\n",
    "# Export files\n",
    "task = \"regression\"\n",
    "for name, graph in graph_splits.items():\n",
    "    print(len(graph))\n",
    "    with open(\"data/graph_data/{}_data/{}.json\".format(task,name), \"w\") as f:\n",
    "        json.dump(graph, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e99e463b7241563ec2935107810be318ced2c0113ddf67d14f4df7287473bd69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('k-adapter-1d2mpP3z-py3.8': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
