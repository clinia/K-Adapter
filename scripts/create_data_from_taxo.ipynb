{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for relation classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script aims to transform our graph triples into training files that can be ingested by the DataProcessors required for this repo. The files created are especially meant to be used for relation classification purposes. Because we don't want to use our NER training sentences, we are going to use the object and the subject of a triple in a next sentence prediction task (<s>object</s></s>subject</s>). The goal here will be to classify the relation between the two segments rather than telling if they are simply related. We hope it's going to help clustering realted triples together in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad formatting, skipping.\n",
      "Bad formatting, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Load graph triples\n",
    "with open(\"data/custom_taxo/clinia_triples.spo\", \"r\") as g:\n",
    "    graph = {}\n",
    "    entities = set()\n",
    "    for line in g:\n",
    "        try:\n",
    "            subj, relation, obj = line.strip().split(\"\\t\")\n",
    "        except:\n",
    "            print(\"Bad formatting, skipping.\")\n",
    "        \n",
    "        entities.add(subj)\n",
    "        entities.add(obj)\n",
    "        if subj not in graph.keys():\n",
    "            graph[subj] = {}\n",
    "            graph[subj][\"relations\"] = [(relation, obj)]\n",
    "        else:\n",
    "            graph[subj][\"relations\"].append((relation, obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Add disjoint triples for negative sampling - FROM RANDOM WORDS\n",
    "\n",
    "with open(\"data/random_words/words.txt\", \"r\") as f:\n",
    "    words = set()\n",
    "    for line in f:\n",
    "        w = line.strip().lower()\n",
    "        if w not in entities:\n",
    "            words.add(w)\n",
    "\n",
    "words = list(words) # Sets are not subscriptable\n",
    "\n",
    "n_triples = 1000 # since we use 2 relations for each subj and we have about 5000 triples\n",
    "disjoint_triples = {}\n",
    "for _ in range(n_triples):\n",
    "    subj = random.choice(words)\n",
    "    obj1 = random.choice(words)\n",
    "    obj2 = random.choice(words)\n",
    "    \n",
    "    if subj != obj1 and subj != obj2 and obj1 != obj2:\n",
    "        disjoint_triples[subj] = {}\n",
    "        disjoint_triples[subj][\"relations\"] = [(\"disjoint with\", obj1), (\"disjoint with\", obj2)]\n",
    "\n",
    "    \n",
    "# Add disjoint triples to graph\n",
    "graph = {**graph, **disjoint_triples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Add disjoint triples for negative sampling - FROM TAXONOMY INSTANCES\n",
    "\n",
    "def verify_triple_existance(subj, obj, graph, reccursion_depth=0):\n",
    "    reccursion_depth += 1\n",
    "    if reccursion_depth == 3:\n",
    "        return False\n",
    "    elif subj not in graph.keys() and obj not in graph.keys():\n",
    "        # search the whole graph if obj and subj are related by a common subject\n",
    "        for s, rel in graph.items():\n",
    "            rel_obj = {item[1] for item in rel[\"relations\"] if item[0] != \"disjoint with\"}\n",
    "            if obj in rel_obj and subj in rel_obj:\n",
    "                return True\n",
    "        return False\n",
    "    elif subj not in graph.keys():\n",
    "            # Check if obj has subj as an object\n",
    "            return verify_triple_existance(obj, subj, graph, reccursion_depth)\n",
    "    elif subj == obj:\n",
    "        return True\n",
    "    else:\n",
    "        rel_obj = {item[1] for item in graph[subj][\"relations\"] if item[0] != \"disjoint with\"}\n",
    "        if obj not in rel_obj and verify_triple_existance(obj, subj, graph, reccursion_depth):\n",
    "            # Check wether obj is within objecs of that subject AND that the opposite triple either doesent exist OR doesnt contain the opposite triple\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "g = {\"test\": {\"relations\":[(\"rel\", \"bbb\"),(\"rel\", \"aaa\")]},\"ccc\": {\"relations\":[(\"rel\", \"ccc\"),(\"disjoint with\", \"ddd\")]}}\n",
    "\n",
    "print(verify_triple_existance(\"test\", \"ccc\", g, 0))\n",
    "\n",
    "words = list(entities) # Sets are not subscriptable\n",
    "\n",
    "n_triples = 2000 # since we use 2 relations for each subj and we have about 5000 triples\n",
    "disjoint_triples = {}\n",
    "for _ in range(n_triples):\n",
    "    subj, obj1, obj2 = random.sample(words, 3)\n",
    "    \n",
    "    # Verify that the triples dont exist\n",
    "    exist_1 = verify_triple_existance(subj, obj1, graph)\n",
    "    exist_2 = verify_triple_existance(subj, obj2, graph)\n",
    "    exist_3 = verify_triple_existance(obj1, obj2, graph)\n",
    "\n",
    "    if not exist_1 and not exist_2 and not exist_3:\n",
    "        if subj not in graph.keys():\n",
    "            graph[subj] = {}\n",
    "            graph[subj][\"relations\"] = [(\"disjoint with\", obj1), (\"disjoint with\", obj2)]\n",
    "        else:\n",
    "            graph[subj][\"relations\"].append((\"disjoint with\", obj1))\n",
    "            graph[subj][\"relations\"].append((\"disjoint with\", obj2))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['range', 'alt label', 'can be service given by institution organization', 'can be person doing intervention', 'is condition treated by institution organization', 'inverse of', 'treats', 'is drug treats condition', 'is discipline treats sign symptom', 'is technology used to perform treatment', 'abbreviation', 'is anatomy affected by condition', 'is partof', 'is condition treated by discipline', 'has primary target population', 'hidden label', 'domain', 'is institution organization treating condition', 'is condition treated by intervention', 'is intervention done by institution organization', 'scope', 'is discipline treats condition', 'disjoint with', 'is sign symptom of condition', 'can be condition treated by institution organization', 'can have target population', 'comment', 'is subspecialty of discipline', 'is discipline of subspecialty', 'is service given by institution organization', 'can be institution organization treating condition', 'is intervention done by person', 'is drug prescribed by person', 'can be intervention done by person', 'feminin', 'studies', 'is person of discipline', 'is condition treated by drug', 'pref label', 'is person does intervention', 'is discipline of person']\n"
     ]
    }
   ],
   "source": [
    "# List relations\n",
    "pred_set = set()\n",
    "for _, relations in graph.items():\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        pred_set.add(predicate)\n",
    "\n",
    "print(list(pred_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903\n",
      "{'disjoint with': 1110, 'alt label': 613, 'is condition treated by intervention': 481, 'treats': 481, 'is condition treated by discipline': 355, 'is discipline treats condition': 355, 'is condition treated by drug': 196, 'is drug treats condition': 195, 'can be person doing intervention': 158, 'can be intervention done by person': 158, 'hidden label': 138, 'is subspecialty of discipline': 105, 'is discipline of subspecialty': 105, 'is person of discipline': 74, 'is discipline of person': 74, 'is drug prescribed by person': 60, 'abbreviation': 48, 'is condition treated by institution organization': 30, 'is institution organization treating condition': 30, 'is sign symptom of condition': 21, 'domain': 20, 'has primary target population': 17, 'range': 15, 'inverse of': 11, 'can be condition treated by institution organization': 10, 'pref label': 10, 'can be institution organization treating condition': 10, 'is anatomy affected by condition': 6, 'is discipline treats sign symptom': 2, 'can have target population': 2, 'scope': 2, 'is service given by institution organization': 2, 'is partof': 1, 'can be service given by institution organization': 1, 'is intervention done by person': 1, 'comment': 1, 'is intervention done by institution organization': 1, 'studies': 1, 'feminin': 1, 'is person does intervention': 1, 'is technology used to perform treatment': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# predicat distribution analysis\n",
    "pred_set = list()\n",
    "for _, relations in graph.items():\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        pred_set.append(predicate)\n",
    "        \n",
    "results = dict(Counter(pred_set))\n",
    "\n",
    "results = dict(sorted(results.items(), key=lambda item: item[1],reverse=True))\n",
    "total = sum([n for key, n in results.items()])\n",
    "\n",
    "print(total)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# predicat distribution analysis\n",
    "obj_set = list()\n",
    "for _, relations in graph.items():\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        obj_set.append(obj)\n",
    "        \n",
    "results = dict(Counter(obj_set))\n",
    "\n",
    "results = dict(sorted(results.items(), key=lambda item: item[1],reverse=True))\n",
    "total = sum([n for key, n in results.items()])\n",
    "\n",
    "print(total)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Instances set for visualization\n",
    "instance_set = set()\n",
    "for subj, relations in graph.items():\n",
    "    instance_set.add(subj)\n",
    "    for predicate, obj in relations[\"relations\"]:\n",
    "        instance_set.add(obj)\n",
    "        \n",
    "df = pd.DataFrame(instance_set)\n",
    "\n",
    "df.to_csv(\"tensorboard/data/facets/all/name.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "90\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Shuffle triples\n",
    "keys = list(graph.keys())\n",
    "random.shuffle(keys)\n",
    "\n",
    "shuffled_graph = {k:graph[k] for k in keys}\n",
    "\n",
    "# Split triples into train, dev and test sets\n",
    "graph_splits = dict()\n",
    "splits = [0.9, 0.1, 0.0]\n",
    "graph_splits[\"train\"] = dict(list(shuffled_graph.items())[: int(len(shuffled_graph) * splits[0])])\n",
    "graph_splits[\"dev\"] = dict(list(shuffled_graph.items())[int(len(shuffled_graph) * splits[0]) : int(len(shuffled_graph) * (splits[0] + splits[1]))])\n",
    "graph_splits[\"test\"] = dict(list(shuffled_graph.items())[int(len(shuffled_graph) * (splits[0] + splits[1])) :])\n",
    "\n",
    "\n",
    "# Export files\n",
    "task = \"mse\"\n",
    "for name, graph in graph_splits.items():\n",
    "    print(len(graph))\n",
    "    with open(\"data/graph_data/{}_data/{}.json\".format(task,name), \"w\") as f:\n",
    "        json.dump(graph, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e99e463b7241563ec2935107810be318ced2c0113ddf67d14f4df7287473bd69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('k-adapter-1d2mpP3z-py3.8': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
